import streamlit as st
from PIL import Image
from langchain_community.chat_models import ChatOllama

from src.config import MODEL_NAME, SERVER_URL
from src.embeddings import build_embeddings
from src.vectorstore import build_vector_store
from src.prompts import QA_PROMPT, DOCUMENT_PROMPT
from src.rag_chain import build_rag_chain
from src.routing_retriever import ActRoutingRetriever
from src.config import RETRIEVER_K


# ---------- Ustawienia strony ----------
st.set_page_config(page_title="Chatbot Prawniczy", layout="wide")

# ==========================================
# STYLE CSS - FINALNA WERSJA (SYMETRIA)
# ==========================================
st.markdown(
    """
    <style>
    /* 1. KONTENER G≈Å√ìWNY */
    [data-testid="stChatInput"] {
        max-width: 800px;           /* Szeroko≈õƒá jak w ChatGPT */
        margin-left: auto;          /* Centrowanie na ekranie */
        margin-right: auto;
        margin-bottom: 40px;        /* Podniesienie nad dolnƒÖ krawƒôd≈∫ */
        
        /* KLUCZOWE DLA SYMETRII: */
        align-items: center !important; /* Wymusza, by wszystko w ≈õrodku (ikony i tekst) by≈Ço w jednej linii poziomej */
        border-radius: 20px;        /* ZaokrƒÖglenie ca≈Çego paska */
    }

    /* 2. POLE TEKSTOWE (≈öRODEK) */
    [data-testid="stChatInput"] textarea {
        min-height: 55px !important;    /* Wysoko≈õƒá fizyczna paska */
        padding-top: 16px !important;   /* Wypychanie tekstu, ≈ºeby by≈Ç na ≈õrodku wysoko≈õci */
        padding-bottom: 16px !important;
    }

    /* 3. PRZYCISKI (IKONA PLIKU + IKONA WYSY≈ÅANIA) */
    /* Ten selektor ≈Çapie ka≈ºdy guzik wewnƒÖtrz paska inputu */
    [data-testid="stChatInput"] button {
        align-self: center !important;  /* Centruje ikonƒô w pionie wzglƒôdem wysokiego paska */
        margin-top: 0px !important;     /* Kasuje ewentualne domy≈õlne przesuniƒôcia Streamlit */
        height: auto !important;
    }
    
    /* Opcjonalnie: Je≈õli ikona pliku jest zbyt blisko krawƒôdzi, dodaj jej margines */
    [data-testid="stChatInputFileUploader"] {
        margin-left: 5px !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)
st.title("ü§ñ Chatbot Prawniczy")


# ---------- Stan sesji (historia chatu + RAG) ----------
if "messages" not in st.session_state:
    # [{"role": "user"/"assistant", "content": str}]
    st.session_state.messages = []

if "rag_ready" not in st.session_state:
    st.session_state.rag_ready = False


# ---------- Inicjalizacja LLM + RAG  ----------
@st.cache_resource(show_spinner=True)
def init_rag():
    llm = ChatOllama(
        base_url=SERVER_URL,   # http://127.0.0.1:11434
        model=MODEL_NAME,      # gemma3:27b-it-q4_K_M
        temperature=0.2,
    )

    embeddings = build_embeddings()
    db, retriever = build_vector_store(embeddings)
    retriever = ActRoutingRetriever(vectorstore=db, k=RETRIEVER_K, max_acts=2, debug=True)

    # Tworzy: rag_chain (retriever+LLM) oraz combine_docs_chain (LLM na podanych docach)
    rag_chain = build_rag_chain(
        llm, retriever, QA_PROMPT, DOCUMENT_PROMPT
    )

    return rag_chain, retriever


if not st.session_state.rag_ready:
    rag_chain, retriever = init_rag()
    st.session_state.rag_chain = rag_chain
    st.session_state.retriever = retriever
    st.session_state.rag_ready = True
else:
    rag_chain = st.session_state.rag_chain
    retriever = st.session_state.retriever


# ---------- Placeholder na tekst z za≈ÇƒÖczonych plik√≥w ----------
def extract_text_from_files(files) -> str:
    """
    TODO:
      - dla PDF: dodaƒá wyciƒÖganie tekstu (PyMuPDF / pdfplumber)
      - dla obraz√≥w: dodaƒá OCR (pytesseract, lang='pol')
    Teraz tylko wypisujemy nazwy plik√≥w jako 'tre≈õƒá wniosku'.
    """
    if not files:
        return ""
    lines = [f"[plik] {f.name}" for f in files]
    return "\n".join(lines)


# ---------- Pipeline RAG dla jednego pytania ----------
def run_rag_pipeline(user_query: str):
    # Pobieramy gotowy ≈Ça≈Ñcuch z sesji
    rag_chain = st.session_state.rag_chain
    
    # Uruchamiamy ≈Ça≈Ñcuch. 
    result = rag_chain.invoke({"input": user_query})

    # WyciƒÖgamy odpowied≈∫
    answer = result.get("answer", "Brak odpowiedzi")

    # WyciƒÖgamy dokumenty, kt√≥re znalaz≈Ç retriever
    retrieved_docs = result.get("context", [])

    return answer.strip(), retrieved_docs, retrieved_docs


# ---------- Wy≈õwietlanie historii chatu ----------
# Je≈õli chcesz, aby wiadomo≈õci te≈º by≈Çy wƒô≈ºsze i na ≈õrodku (jak w ChatGPT),
# mo≈ºesz odkomentowaƒá styl .stChatMessage w sekcji CSS powy≈ºej.
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])


# ---------- Nowa wiadomo≈õƒá u≈ºytkownika + za≈ÇƒÖczniki przy input ----------
chat_value = st.chat_input(
    "Zadaj pytanie lub napisz polecenie...",
    accept_file="multiple",
    file_type=["pdf", "png", "jpg", "jpeg"],
)

if chat_value is not None:
    # chat_value to obiekt ChatInputValue: ma .text i .files
    user_text = chat_value.text or ""
    user_files = chat_value.files or []

    # je≈õli jest jakikolwiek tekst albo pliki, to dzia≈Çamy
    if user_text.strip() or user_files:
        # 1) dopisz wiadomo≈õƒá u≈ºytkownika do historii
        st.session_state.messages.append(
            {"role": "user", "content": user_text}
        )
        with st.chat_message("user"):
            st.markdown(user_text if user_text.strip() else "[wiadomo≈õƒá z za≈ÇƒÖcznikami]")

        # 2) prosty ‚Äûwniosek‚Äù z plik√≥w (na razie tylko nazwy)
        wniosek_text = extract_text_from_files(user_files)
        if wniosek_text:
            with st.expander("Za≈ÇƒÖczone pliki (do analizy wniosku)"):
                st.text(wniosek_text)

        # 3) RAG + debug + odpowied≈∫
        with st.chat_message("assistant"):
            with st.spinner("Analizujƒô dokumenty..."):
                answer_text, raw_docs, final_docs = run_rag_pipeline(user_text)

                st.markdown(answer_text)

                st.markdown("**≈πr√≥d≈Ça:**")
                if not final_docs:
                    st.write("- Brak ≈∫r√≥de≈Ç.")
                else:
                    for doc in final_docs:
                        src = doc.metadata.get("source", "Nieznane ≈∫r√≥d≈Ço")
                        page = doc.metadata.get("page", "N/A")
                        st.write(f"- {src}, strona {page}")

                with st.expander("Informacje debugowe (retriever)", expanded=False):
                    st.subheader("Krok 1: Surowe wyniki z bazy wektorowej (raw_docs)")
                    for i, doc in enumerate(raw_docs):
                        st.write(
                            f"**Wynik [RAW] #{i}** "
                            f"(Source: {doc.metadata.get('source')}, "
                            f"Page: {doc.metadata.get('page')})"
                        )
                        st.text(f"{doc.page_content[:500]}...")

                    st.subheader("Krok 2: Wyniki ko≈Ñcowe(final_docs)")
                    for i, doc in enumerate(final_docs):
                        st.write(
                            f"**Wynik [FINAL] #{i}** "
                            f"(Source: {doc.metadata.get('source')}, "
                            f"Page: {doc.metadata.get('page')})"
                        )
                        st.text(f"{doc.page_content[:500]}...")

        # 4) zapisz odpowied≈∫ w historii
        st.session_state.messages.append(
            {"role": "assistant", "content": answer_text}
        )